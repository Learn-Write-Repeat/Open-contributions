{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Rock Paper Scissor Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten , Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import cv2\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"rps/rps\"\n",
    "test_dir = \"rps-test-set/rps-test-set\"\n",
    "\n",
    "train_rock = os.listdir(train_dir +\"/rock\")\n",
    "train_paper = os.listdir(train_dir +\"/paper\")\n",
    "train_scissors = os.listdir(train_dir +\"/scissors\")\n",
    "\n",
    "test_rock = os.listdir(train_dir +\"/rock\")\n",
    "test_scissors = os.listdir(train_dir +\"/scissors\")\n",
    "test_paper = os.listdir(train_dir +\"/paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the train-set: 2520\n",
      "Number of images in the test-set: 2520\n",
      "\n",
      "Number of rocks in the train-set: 840\n",
      "Number of papers in the train-set: 840\n",
      "Number of scissors in the train-set: 840\n",
      "\n",
      "Number of rocks in the test-set: 840\n",
      "Number of papers in the test-set: 840\n",
      "Number of scissors in the test-set: 840\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images in the train-set:\", len(train_rock) + len(train_paper) + len(train_scissors))\n",
    "print(\"Number of images in the test-set:\", len(test_rock) + len(test_paper) + len(test_scissors))\n",
    "\n",
    "print(\"\\nNumber of rocks in the train-set:\", len(train_rock))\n",
    "print(\"Number of papers in the train-set:\", len(train_paper))\n",
    "print(\"Number of scissors in the train-set:\", len(train_scissors))\n",
    "\n",
    "print(\"\\nNumber of rocks in the test-set:\", len(test_rock))\n",
    "print(\"Number of papers in the test-set:\", len(test_paper))\n",
    "print(\"Number of scissors in the test-set:\", len(test_scissors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n",
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen= ImageDataGenerator(rescale= 1/255.0,\n",
    "                           width_shift_range =0.2 ,\n",
    "                           height_shift_range=0.2 ,\n",
    "                           zoom_range=0.2 ,\n",
    "                           rotation_range=40 ,\n",
    "                           shear_range=0.2, \n",
    "                           horizontal_flip= True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale= 1/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(75,75), class_mode=\"categorical\", batch_size=128)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(75,75), class_mode=\"categorical\", batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Convolution Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,573,443\n",
      "Trainable params: 1,573,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\", input_shape=(75,75,3), padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, activation= \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of keras callbacks\n",
    "\n",
    "my_callbacks_es = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "my_callbacks_rlr = ReduceLROnPlateau(monitor='val_accuracy', pateince=2, factor=0.5, min_lr=0.00001, verbose=1)\n",
    "my_callbacks_mc = ModelCheckpoint(\"model.h5\",monitor='val_accuracy', save_best_only=True, verbose=1, mode='max')\n",
    "my_callbacks = [my_callbacks_es, my_callbacks_rlr, my_callbacks_mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile model\n",
    "\n",
    "model.compile(loss= \"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 117s 6s/step - loss: 1.1102 - accuracy: 0.3385 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33333, saving model to model.h5\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 68s 3s/step - loss: 1.0909 - accuracy: 0.3821 - val_loss: 1.0193 - val_accuracy: 0.4435\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.33333 to 0.44355, saving model to model.h5\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 68s 3s/step - loss: 0.9180 - accuracy: 0.5361 - val_loss: 0.7387 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.44355 to 0.80376, saving model to model.h5\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 67s 3s/step - loss: 0.7174 - accuracy: 0.6405 - val_loss: 0.4374 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.80376 to 0.83333, saving model to model.h5\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 69s 3s/step - loss: 0.5704 - accuracy: 0.7321 - val_loss: 0.4114 - val_accuracy: 0.7608\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.83333\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 68s 3s/step - loss: 0.4375 - accuracy: 0.8095 - val_loss: 0.1555 - val_accuracy: 0.8737\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.83333 to 0.87366, saving model to model.h5\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.2879 - accuracy: 0.8925 - val_loss: 0.1012 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.87366 to 0.93817, saving model to model.h5\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 71s 4s/step - loss: 0.2163 - accuracy: 0.9222 - val_loss: 0.0739 - val_accuracy: 0.9704\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.93817 to 0.97043, saving model to model.h5\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 78s 4s/step - loss: 0.1173 - accuracy: 0.9611 - val_loss: 0.0919 - val_accuracy: 0.9570\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.97043\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.1148 - accuracy: 0.9587 - val_loss: 0.0390 - val_accuracy: 0.9704\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.97043\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.1282 - accuracy: 0.9480 - val_loss: 0.0720 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.97043 to 0.97849, saving model to model.h5\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 70s 4s/step - loss: 0.1125 - accuracy: 0.9615 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.97849 to 1.00000, saving model to model.h5\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 72s 4s/step - loss: 0.0988 - accuracy: 0.9659 - val_loss: 0.0515 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 1.00000\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 72s 4s/step - loss: 0.0524 - accuracy: 0.9790 - val_loss: 0.0402 - val_accuracy: 0.9892\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 1.00000\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.0588 - accuracy: 0.9766 - val_loss: 0.0465 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 1.00000\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 53s 3s/step - loss: 0.0872 - accuracy: 0.9687 - val_loss: 0.0499 - val_accuracy: 0.9651\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 1.00000\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 52s 3s/step - loss: 0.0411 - accuracy: 0.9853 - val_loss: 0.0494 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 1.00000\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "## train model\n",
    "\n",
    "history= model.fit(train_generator, validation_data=test_generator, \n",
    "                  epochs = 25, verbose=1, steps_per_epoch= 20, validation_steps=3,\n",
    "                  callbacks= my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load our trained model \n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using OpenCV to test model with real time video from webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start webcam\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "start = True\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    img =  copy.deepcopy(frame)\n",
    "    \n",
    "    cv2.rectangle(img, (20,50), (320, 350), (255,0,0), 3)\n",
    "    \n",
    "    roi = frame[40:340, 10:310] \n",
    "    \n",
    "    input_img = cv2.resize(roi, (75,75))\n",
    "    \n",
    "    \n",
    "    input_img = np.expand_dims(input_img, axis=0)\n",
    "    list1 = ['PAPER', \"ROCK\", \"SCISSORS\"]\n",
    "    array= model.predict(input_img)\n",
    "    id= np.argmax(array)\n",
    "    text = list1[id]\n",
    "    cv2.putText(img, \"Put your hand in the box\", (20,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,255), 2)\n",
    "    cv2.putText(img, text, (20,380), cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,255,255), 2)\n",
    "    cv2.imshow(\"frame\", img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key== ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
