{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.8 64-bit",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e2d4075660d1d643d1328920576ea01a1bd48051b463c92d735ed4cbd11b4f6e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Implementing Word2Vec Model using Skip-Gram "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing the Necesssary Stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/Skip-gram-architecture-2.jpg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's define some variables :\n",
    "\n",
    "V    Number of unique words in our corpus of text ( Vocabulary )<br>\n",
    "x    Input layer (One hot encoding of our input word ). <br>\n",
    "N    Number of neurons in the hidden layer of neural network<br>\n",
    "W    Weights between input layer and hidden layer<br>\n",
    "W'   Weights between hidden layer and output layer<br>\n",
    "y    A softmax output layer having probabilities of every word in our vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Softmax Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum() \n",
    "   "
   ]
  },
  {
   "source": [
    "## Word2Vec Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Contains Functions for Forward Propogation, Backward Propogation, Training and Predicting the words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In <u><b>Forward Propogation</b></u>, We multiply one hot encoding of centre word (denoted by x) with the first weight matrix W to get hidden layer matrix h (of size N x 1). We then multiply the hidden layer vector h with second weight matrix W’ to get a new matrix u. We Then obtain our loss function, Which comes out to be <br><img src=\"https://miro.medium.com/max/994/1*XPhzBnf1xEb0u67qazx9nA.png\"><br>\n",
    "E being our Loss Function.<br>\n",
    "In <u><b>Backward Propogation</b></u>, We find the partial derivatives of our loss function with respect to W and W’ to apply gradient descent algorithm.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.N = 10\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        self.words = [] \n",
    "        self.word_index = {} \n",
    "   \n",
    "    def initialize(self,V,data): \n",
    "        self.V = V \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data \n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i \n",
    "   \n",
    "       \n",
    "    def feed_forward(self,X): \n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W1.T,self.h) \n",
    "        #print(self.u) \n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x,t): \n",
    "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
    "        # e.shape is V x 1 \n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW \n",
    "           \n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)): \n",
    "                self.feed_forward(self.X_train[j]) \n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\") "
   ]
  },
  {
   "source": [
    "## Functions for Preparing and Preprocessing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus): \n",
    "    stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\".\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     if word not in stop_words] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data \n",
    "       \n",
    "   \n",
    "def prepare_data_for_training(sentences,w2v): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "    vocab = {} \n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    #for i in range(len(words)): \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "   \n",
    "    return w2v.X_train,w2v.y_train "
   ]
  },
  {
   "source": [
    "## Source Code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.29210593184752\nepoch  484  loss =  141.29056401378705\nepoch  485  loss =  141.28902844762388\nepoch  486  loss =  141.2874991942975\nepoch  487  loss =  141.2859762150668\nepoch  488  loss =  141.2844594715055\nepoch  489  loss =  141.28294892550002\nepoch  490  loss =  141.28144453924565\nepoch  491  loss =  141.2799462752441\nepoch  492  loss =  141.2784540962994\nepoch  493  loss =  141.2769679655161\nepoch  494  loss =  141.2754878462951\nepoch  495  loss =  141.2740137023315\nepoch  496  loss =  141.2725454976114\nepoch  497  loss =  141.2710831964087\nepoch  498  loss =  141.2696267632827\nepoch  499  loss =  141.26817616307508\nepoch  500  loss =  141.26673136090713\nepoch  501  loss =  141.26529232217675\nepoch  502  loss =  141.26385901255594\nepoch  503  loss =  141.26243139798834\nepoch  504  loss =  141.2610094446858\nepoch  505  loss =  141.25959311912683\nepoch  506  loss =  141.25818238805297\nepoch  507  loss =  141.2567772184668\nepoch  508  loss =  141.255377577629\nepoch  509  loss =  141.2539834330565\nepoch  510  loss =  141.2525947525191\nepoch  511  loss =  141.25121150403788\nepoch  512  loss =  141.24983365588196\nepoch  513  loss =  141.2484611765669\nepoch  514  loss =  141.24709403485153\nepoch  515  loss =  141.24573219973627\nepoch  516  loss =  141.24437564046045\nepoch  517  loss =  141.2430243265001\nepoch  518  loss =  141.24167822756576\nepoch  519  loss =  141.2403373135999\nepoch  520  loss =  141.23900155477537\nepoch  521  loss =  141.23767092149242\nepoch  522  loss =  141.23634538437733\nepoch  523  loss =  141.23502491427953\nepoch  524  loss =  141.23370948226994\nepoch  525  loss =  141.23239905963877\nepoch  526  loss =  141.23109361789332\nepoch  527  loss =  141.2297931287563\nepoch  528  loss =  141.22849756416343\nepoch  529  loss =  141.2272068962614\nepoch  530  loss =  141.22592109740626\nepoch  531  loss =  141.2246401401613\nepoch  532  loss =  141.22336399729494\nepoch  533  loss =  141.2220926417791\nepoch  534  loss =  141.220826046787\nepoch  535  loss =  141.2195641856916\nepoch  536  loss =  141.2183070320636\nepoch  537  loss =  141.21705455966963\nepoch  538  loss =  141.21580674247045\nepoch  539  loss =  141.21456355461896\nepoch  540  loss =  141.21332497045887\nepoch  541  loss =  141.21209096452262\nepoch  542  loss =  141.21086151152966\nepoch  543  loss =  141.20963658638496\nepoch  544  loss =  141.208416164177\nepoch  545  loss =  141.20720022017645\nepoch  546  loss =  141.20598872983408\nepoch  547  loss =  141.20478166877956\nepoch  548  loss =  141.20357901281972\nepoch  549  loss =  141.2023807379367\nepoch  550  loss =  141.2011868202866\nepoch  551  loss =  141.19999723619793\nepoch  552  loss =  141.19881196216988\nepoch  553  loss =  141.197630974871\nepoch  554  loss =  141.1964542511375\nepoch  555  loss =  141.19528176797178\nepoch  556  loss =  141.19411350254111\nepoch  557  loss =  141.1929494321762\nepoch  558  loss =  141.19178953436904\nepoch  559  loss =  141.1906337867725\nepoch  560  loss =  141.18948216719826\nepoch  561  loss =  141.18833465361547\nepoch  562  loss =  141.18719122414956\nepoch  563  loss =  141.1860518570808\nepoch  564  loss =  141.18491653084263\nepoch  565  loss =  141.18378522402088\nepoch  566  loss =  141.18265791535185\nepoch  567  loss =  141.18153458372137\nepoch  568  loss =  141.1804152081634\nepoch  569  loss =  141.17929976785862\nepoch  570  loss =  141.17818824213344\nepoch  571  loss =  141.17708061045823\nepoch  572  loss =  141.1759768524467\nepoch  573  loss =  141.17487694785405\nepoch  574  loss =  141.17378087657636\nepoch  575  loss =  141.17268861864864\nepoch  576  loss =  141.1716001542442\nepoch  577  loss =  141.1705154636734\nepoch  578  loss =  141.16943452738224\nepoch  579  loss =  141.16835732595135\nepoch  580  loss =  141.1672838400947\nepoch  581  loss =  141.16621405065877\nepoch  582  loss =  141.16514793862103\nepoch  583  loss =  141.16408548508903\nepoch  584  loss =  141.16302667129935\nepoch  585  loss =  141.16197147861624\nepoch  586  loss =  141.16091988853097\nepoch  587  loss =  141.15987188266016\nepoch  588  loss =  141.15882744274532\nepoch  589  loss =  141.15778655065154\nepoch  590  loss =  141.1567491883662\nepoch  591  loss =  141.15571533799834\nepoch  592  loss =  141.15468498177754\nepoch  593  loss =  141.1536581020528\nepoch  594  loss =  141.15263468129152\nepoch  595  loss =  141.15161470207846\nepoch  596  loss =  141.1505981471153\nepoch  597  loss =  141.14958499921877\nepoch  598  loss =  141.14857524132043\nepoch  599  loss =  141.14756885646543\nepoch  600  loss =  141.14656582781146\nepoch  601  loss =  141.14556613862817\nepoch  602  loss =  141.1445697722958\nepoch  603  loss =  141.14357671230462\nepoch  604  loss =  141.14258694225387\nepoch  605  loss =  141.14160044585077\nepoch  606  loss =  141.14061720690998\nepoch  607  loss =  141.1396372093524\nepoch  608  loss =  141.1386604372042\nepoch  609  loss =  141.13768687459654\nepoch  610  loss =  141.1367165057639\nepoch  611  loss =  141.13574931504397\nepoch  612  loss =  141.13478528687637\nepoch  613  loss =  141.13382440580193\nepoch  614  loss =  141.1328666564621\nepoch  615  loss =  141.1319120235977\nepoch  616  loss =  141.1309604920484\nepoch  617  loss =  141.13001204675197\nepoch  618  loss =  141.12906667274325\nepoch  619  loss =  141.12812435515366\nepoch  620  loss =  141.12718507921025\nepoch  621  loss =  141.12624883023483\nepoch  622  loss =  141.1253155936434\nepoch  623  loss =  141.12438535494556\nepoch  624  loss =  141.12345809974312\nepoch  625  loss =  141.12253381373023\nepoch  626  loss =  141.12161248269172\nepoch  627  loss =  141.1206940925034\nepoch  628  loss =  141.11977862913042\nepoch  629  loss =  141.11886607862715\nepoch  630  loss =  141.117956427136\nepoch  631  loss =  141.11704966088743\nepoch  632  loss =  141.11614576619834\nepoch  633  loss =  141.11524472947232\nepoch  634  loss =  141.11434653719814\nepoch  635  loss =  141.11345117594968\nepoch  636  loss =  141.112558632385\nepoch  637  loss =  141.1116688932458\nepoch  638  loss =  141.1107819453565\nepoch  639  loss =  141.10989777562392\nepoch  640  loss =  141.10901637103666\nepoch  641  loss =  141.108137718664\nepoch  642  loss =  141.10726180565584\nepoch  643  loss =  141.1063886192417\nepoch  644  loss =  141.1055181467303\nepoch  645  loss =  141.10465037550875\nepoch  646  loss =  141.1037852930423\nepoch  647  loss =  141.10292288687327\nepoch  648  loss =  141.10206314462087\nepoch  649  loss =  141.10120605398026\nepoch  650  loss =  141.10035160272244\nepoch  651  loss =  141.09949977869314\nepoch  652  loss =  141.09865056981263\nepoch  653  loss =  141.09780396407467\nepoch  654  loss =  141.09695994954683\nepoch  655  loss =  141.09611851436898\nepoch  656  loss =  141.0952796467532\nepoch  657  loss =  141.09444333498337\nepoch  658  loss =  141.09360956741438\nepoch  659  loss =  141.0927783324714\nepoch  660  loss =  141.09194961864986\nepoch  661  loss =  141.09112341451464\nepoch  662  loss =  141.09029970869955\nepoch  663  loss =  141.08947848990672\nepoch  664  loss =  141.0886597469063\nepoch  665  loss =  141.0878434685358\nepoch  666  loss =  141.08702964369962\nepoch  667  loss =  141.08621826136863\nepoch  668  loss =  141.0854093105798\nepoch  669  loss =  141.08460278043492\nepoch  670  loss =  141.08379866010134\nepoch  671  loss =  141.08299693881045\nepoch  672  loss =  141.08219760585786\nepoch  673  loss =  141.08140065060257\nepoch  674  loss =  141.0806060624666\nepoch  675  loss =  141.0798138309346\nepoch  676  loss =  141.07902394555316\nepoch  677  loss =  141.07823639593076\nepoch  678  loss =  141.07745117173693\nepoch  679  loss =  141.07666826270187\nepoch  680  loss =  141.0758876586162\nepoch  681  loss =  141.07510934933035\nepoch  682  loss =  141.07433332475424\nepoch  683  loss =  141.0735595748565\nepoch  684  loss =  141.07278808966478\nepoch  685  loss =  141.0720188592645\nepoch  686  loss =  141.0712518737988\nepoch  687  loss =  141.07048712346835\nepoch  688  loss =  141.0697245985304\nepoch  689  loss =  141.0689642892989\nepoch  690  loss =  141.06820618614364\nepoch  691  loss =  141.0674502794904\nepoch  692  loss =  141.06669655981972\nepoch  693  loss =  141.0659450176674\nepoch  694  loss =  141.06519564362347\nepoch  695  loss =  141.06444842833227\nepoch  696  loss =  141.06370336249148\nepoch  697  loss =  141.06296043685234\nepoch  698  loss =  141.06221964221882\nepoch  699  loss =  141.06148096944764\nepoch  700  loss =  141.06074440944747\nepoch  701  loss =  141.060009953179\nepoch  702  loss =  141.05927759165417\nepoch  703  loss =  141.0585473159359\nepoch  704  loss =  141.057819117138\nepoch  705  loss =  141.0570929864247\nepoch  706  loss =  141.05636891500984\nepoch  707  loss =  141.0556468941572\nepoch  708  loss =  141.05492691517983\nepoch  709  loss =  141.05420896943957\nepoch  710  loss =  141.05349304834695\nepoch  711  loss =  141.05277914336074\nepoch  712  loss =  141.05206724598764\nepoch  713  loss =  141.05135734778176\nepoch  714  loss =  141.05064944034478\nepoch  715  loss =  141.04994351532503\nepoch  716  loss =  141.04923956441755\nepoch  717  loss =  141.0485375793635\nepoch  718  loss =  141.04783755195024\nepoch  719  loss =  141.04713947401046\nepoch  720  loss =  141.04644333742246\nepoch  721  loss =  141.04574913410931\nepoch  722  loss =  141.04505685603874\nepoch  723  loss =  141.0443664952232\nepoch  724  loss =  141.04367804371878\nepoch  725  loss =  141.04299149362558\nepoch  726  loss =  141.04230683708715\nepoch  727  loss =  141.04162406629013\nepoch  728  loss =  141.04094317346411\nepoch  729  loss =  141.0402641508812\nepoch  730  loss =  141.03958699085595\nepoch  731  loss =  141.03891168574452\nepoch  732  loss =  141.03823822794536\nepoch  733  loss =  141.0375666098979\nepoch  734  loss =  141.03689682408287\nepoch  735  loss =  141.03622886302188\nepoch  736  loss =  141.0355627192773\nepoch  737  loss =  141.03489838545127\nepoch  738  loss =  141.03423585418668\nepoch  739  loss =  141.03357511816574\nepoch  740  loss =  141.03291617011024\nepoch  741  loss =  141.03225900278136\nepoch  742  loss =  141.0316036089791\nepoch  743  loss =  141.03094998154225\nepoch  744  loss =  141.03029811334812\nepoch  745  loss =  141.029647997312\nepoch  746  loss =  141.02899962638745\nepoch  747  loss =  141.02835299356542\nepoch  748  loss =  141.02770809187442\nepoch  749  loss =  141.02706491438013\nepoch  750  loss =  141.02642345418533\nepoch  751  loss =  141.02578370442916\nepoch  752  loss =  141.02514565828767\nepoch  753  loss =  141.02450930897254\nepoch  754  loss =  141.02387464973202\nepoch  755  loss =  141.02324167384967\nepoch  756  loss =  141.02261037464476\nepoch  757  loss =  141.0219807454718\nepoch  758  loss =  141.0213527797203\nepoch  759  loss =  141.0207264708144\nepoch  760  loss =  141.02010181221317\nepoch  761  loss =  141.01947879740976\nepoch  762  loss =  141.01885741993166\nepoch  763  loss =  141.01823767333985\nepoch  764  loss =  141.0176195512295\nepoch  765  loss =  141.017003047229\nepoch  766  loss =  141.01638815499985\nepoch  767  loss =  141.01577486823678\nepoch  768  loss =  141.01516318066723\nepoch  769  loss =  141.0145530860513\nepoch  770  loss =  141.0139445781814\nepoch  771  loss =  141.01333765088225\nepoch  772  loss =  141.01273229801026\nepoch  773  loss =  141.0121285134539\nepoch  774  loss =  141.01152629113312\nepoch  775  loss =  141.0109256249991\nepoch  776  loss =  141.01032650903437\nepoch  777  loss =  141.00972893725225\nepoch  778  loss =  141.00913290369667\nepoch  779  loss =  141.00853840244253\nepoch  780  loss =  141.00794542759485\nepoch  781  loss =  141.00735397328876\nepoch  782  loss =  141.00676403368948\nepoch  783  loss =  141.00617560299202\nepoch  784  loss =  141.00558867542088\nepoch  785  loss =  141.00500324522997\nepoch  786  loss =  141.00441930670257\nepoch  787  loss =  141.0038368541509\nepoch  788  loss =  141.00325588191592\nepoch  789  loss =  141.0026763843674\nepoch  790  loss =  141.0020983559037\nepoch  791  loss =  141.0015217909512\nepoch  792  loss =  141.00094668396457\nepoch  793  loss =  141.0003730294264\nepoch  794  loss =  140.9998008218472\nepoch  795  loss =  140.99923005576483\nepoch  796  loss =  140.99866072574466\nepoch  797  loss =  140.99809282637938\nepoch  798  loss =  140.99752635228884\nepoch  799  loss =  140.99696129811957\nepoch  800  loss =  140.99639765854513\nepoch  801  loss =  140.99583542826542\nepoch  802  loss =  140.9952746020069\nepoch  803  loss =  140.99471517452218\nepoch  804  loss =  140.99415714059006\nepoch  805  loss =  140.99360049501524\nepoch  806  loss =  140.99304523262805\nepoch  807  loss =  140.9924913482847\nepoch  808  loss =  140.99193883686664\nepoch  809  loss =  140.9913876932806\nepoch  810  loss =  140.99083791245857\nepoch  811  loss =  140.99028948935745\nepoch  812  loss =  140.9897424189591\nepoch  813  loss =  140.98919669626974\nepoch  814  loss =  140.98865231632044\nepoch  815  loss =  140.98810927416642\nepoch  816  loss =  140.98756756488737\nepoch  817  loss =  140.9870271835868\nepoch  818  loss =  140.98648812539224\nepoch  819  loss =  140.98595038545508\nepoch  820  loss =  140.98541395895032\nepoch  821  loss =  140.98487884107632\nepoch  822  loss =  140.98434502705496\nepoch  823  loss =  140.98381251213135\nepoch  824  loss =  140.9832812915734\nepoch  825  loss =  140.98275136067235\nepoch  826  loss =  140.98222271474194\nepoch  827  loss =  140.98169534911852\nepoch  828  loss =  140.98116925916128\nepoch  829  loss =  140.98064444025158\nepoch  830  loss =  140.98012088779296\nepoch  831  loss =  140.97959859721124\nepoch  832  loss =  140.979077563954\nepoch  833  loss =  140.97855778349088\nepoch  834  loss =  140.97803925131333\nepoch  835  loss =  140.9775219629341\nepoch  836  loss =  140.9770059138875\nepoch  837  loss =  140.97649109972917\nepoch  838  loss =  140.9759775160361\nepoch  839  loss =  140.9754651584063\nepoch  840  loss =  140.9749540224585\nepoch  841  loss =  140.97444410383255\nepoch  842  loss =  140.97393539818881\nepoch  843  loss =  140.97342790120848\nepoch  844  loss =  140.97292160859274\nepoch  845  loss =  140.97241651606376\nepoch  846  loss =  140.97191261936342\nepoch  847  loss =  140.97140991425385\nepoch  848  loss =  140.97090839651725\nepoch  849  loss =  140.9704080619556\nepoch  850  loss =  140.9699089063906\nepoch  851  loss =  140.96941092566368\nepoch  852  loss =  140.9689141156357\nepoch  853  loss =  140.96841847218695\nepoch  854  loss =  140.96792399121708\nepoch  855  loss =  140.96743066864482\nepoch  856  loss =  140.966938500408\nepoch  857  loss =  140.9664474824634\nepoch  858  loss =  140.96595761078663\nepoch  859  loss =  140.96546888137206\nepoch  860  loss =  140.9649812902327\nepoch  861  loss =  140.96449483340007\nepoch  862  loss =  140.96400950692404\nepoch  863  loss =  140.9635253068728\nepoch  864  loss =  140.9630422293329\nepoch  865  loss =  140.96256027040872\nepoch  866  loss =  140.96207942622272\nepoch  867  loss =  140.96159969291543\nepoch  868  loss =  140.96112106664492\nepoch  869  loss =  140.960643543587\nepoch  870  loss =  140.96016711993522\nepoch  871  loss =  140.9596917919003\nepoch  872  loss =  140.95921755571072\nepoch  873  loss =  140.95874440761193\nepoch  874  loss =  140.9582723438665\nepoch  875  loss =  140.9578013607545\nepoch  876  loss =  140.95733145457254\nepoch  877  loss =  140.9568626216345\nepoch  878  loss =  140.95639485827078\nepoch  879  loss =  140.95592816082834\nepoch  880  loss =  140.95546252567124\nepoch  881  loss =  140.95499794917959\nepoch  882  loss =  140.95453442775016\nepoch  883  loss =  140.95407195779586\nepoch  884  loss =  140.95361053574604\nepoch  885  loss =  140.95315015804601\nepoch  886  loss =  140.9526908211572\nepoch  887  loss =  140.95223252155688\nepoch  888  loss =  140.9517752557385\nepoch  889  loss =  140.95131902021095\nepoch  890  loss =  140.95086381149895\nepoch  891  loss =  140.9504096261428\nepoch  892  loss =  140.94995646069833\nepoch  893  loss =  140.94950431173675\nepoch  894  loss =  140.94905317584463\nepoch  895  loss =  140.9486030496239\nepoch  896  loss =  140.94815392969144\nepoch  897  loss =  140.94770581267943\nepoch  898  loss =  140.94725869523484\nepoch  899  loss =  140.9468125740198\nepoch  900  loss =  140.9463674457111\nepoch  901  loss =  140.94592330700033\nepoch  902  loss =  140.9454801545937\nepoch  903  loss =  140.9450379852121\nepoch  904  loss =  140.94459679559085\nepoch  905  loss =  140.94415658247988\nepoch  906  loss =  140.94371734264317\nepoch  907  loss =  140.94327907285918\nepoch  908  loss =  140.94284176992053\nepoch  909  loss =  140.94240543063376\nepoch  910  loss =  140.94197005181988\nepoch  911  loss =  140.94153563031347\nepoch  912  loss =  140.941102162963\nepoch  913  loss =  140.94066964663105\nepoch  914  loss =  140.9402380781936\nepoch  915  loss =  140.9398074545405\nepoch  916  loss =  140.9393777725751\nepoch  917  loss =  140.93894902921429\nepoch  918  loss =  140.93852122138833\nepoch  919  loss =  140.93809434604094\nepoch  920  loss =  140.93766840012898\nepoch  921  loss =  140.93724338062262\nepoch  922  loss =  140.9368192845053\nepoch  923  loss =  140.93639610877327\nepoch  924  loss =  140.935973850436\nepoch  925  loss =  140.93555250651573\nepoch  926  loss =  140.93513207404766\nepoch  927  loss =  140.9347125500799\nepoch  928  loss =  140.93429393167298\nepoch  929  loss =  140.93387621590034\nepoch  930  loss =  140.93345939984786\nepoch  931  loss =  140.93304348061415\nepoch  932  loss =  140.93262845530998\nepoch  933  loss =  140.93221432105867\nepoch  934  loss =  140.93180107499592\nepoch  935  loss =  140.93138871426962\nepoch  936  loss =  140.93097723603978\nepoch  937  loss =  140.93056663747856\nepoch  938  loss =  140.93015691577034\nepoch  939  loss =  140.92974806811125\nepoch  940  loss =  140.9293400917095\nepoch  941  loss =  140.9289329837852\nepoch  942  loss =  140.92852674157012\nepoch  943  loss =  140.92812136230796\nepoch  944  loss =  140.9277168432539\nepoch  945  loss =  140.9273131816749\nepoch  946  loss =  140.92691037484937\nepoch  947  loss =  140.92650842006734\nepoch  948  loss =  140.92610731463012\nepoch  949  loss =  140.9257070558505\nepoch  950  loss =  140.92530764105254\nepoch  951  loss =  140.92490906757158\nepoch  952  loss =  140.92451133275424\nepoch  953  loss =  140.92411443395812\nepoch  954  loss =  140.92371836855202\nepoch  955  loss =  140.92332313391555\nepoch  956  loss =  140.9229287274395\nepoch  957  loss =  140.92253514652583\nepoch  958  loss =  140.9221423885867\nepoch  959  loss =  140.9217504510455\nepoch  960  loss =  140.92135933133622\nepoch  961  loss =  140.92096902690363\nepoch  962  loss =  140.92057953520307\nepoch  963  loss =  140.92019085370052\nepoch  964  loss =  140.91980297987223\nepoch  965  loss =  140.91941591120516\nepoch  966  loss =  140.91902964519667\nepoch  967  loss =  140.91864417935435\nepoch  968  loss =  140.9182595111962\nepoch  969  loss =  140.91787563825034\nepoch  970  loss =  140.91749255805524\nepoch  971  loss =  140.91711026815946\nepoch  972  loss =  140.9167287661215\nepoch  973  loss =  140.9163480495102\nepoch  974  loss =  140.9159681159041\nepoch  975  loss =  140.91558896289175\nepoch  976  loss =  140.91521058807177\nepoch  977  loss =  140.9148329890523\nepoch  978  loss =  140.91445616345155\nepoch  979  loss =  140.9140801088973\nepoch  980  loss =  140.91370482302705\nepoch  981  loss =  140.913330303488\nepoch  982  loss =  140.91295654793686\nepoch  983  loss =  140.9125835540398\nepoch  984  loss =  140.91221131947276\nepoch  985  loss =  140.91183984192082\nepoch  986  loss =  140.91146911907856\nepoch  987  loss =  140.91109914864998\nepoch  988  loss =  140.9107299283482\nepoch  989  loss =  140.91036145589592\nepoch  990  loss =  140.90999372902465\nepoch  991  loss =  140.90962674547532\nepoch  992  loss =  140.90926050299788\nepoch  993  loss =  140.90889499935128\nepoch  994  loss =  140.90853023230358\nepoch  995  loss =  140.90816619963203\nepoch  996  loss =  140.9078028991222\nepoch  997  loss =  140.90744032856918\nepoch  998  loss =  140.9070784857766\nepoch  999  loss =  140.9067173685569\n['originated', 'anarchism', 'class']\n"
    }
   ],
   "source": [
    "corpus = \"\" \n",
    "corpus += \"anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution\"\n",
    "epochs = 1000\n",
    "  \n",
    "training_data = preprocessing(corpus) \n",
    "w2v = word2vec() \n",
    "  \n",
    "prepare_data_for_training(training_data,w2v) \n",
    "w2v.train(epochs)  \n",
    "  \n",
    "print(w2v.predict(\"term\",3))     "
   ]
  },
  {
   "source": [
    "The Model Predicts 'anarchism', 'originated' and 'class' to have similar meaning to that of 'term' which is somewhat true.<br>\n",
    "Hence we can say that this model is Functional."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
