{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwSlo4nidbbg"
   },
   "source": [
    "# The Word2Vec Model\n",
    "Welcome! Through the following lines of code, I have attempted to demonstrate the Word2Vec NN model using pytorch. You would actually need a much larger training corpus (>=10,000 words), than the one used in this code, in order to obtain a reasonably accurate set of word embeddings. You would also require a different type of softmax layer (hierarchical softmax) for faster computation. \n",
    "\n",
    "If you wish to modify certain parameters I would recommend following the guidelines given before each code cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4801,
     "status": "ok",
     "timestamp": 1601226337676,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "jK1FBHYWAYRH"
   },
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpBcHH-xhsap"
   },
   "source": [
    "### The Training Corpus\n",
    "\n",
    "\n",
    "*   Defines the list containing the sentences that the model is to be trained on.\n",
    "*   You can add/ edit/ delete sentences as per your preference, however DO NOT use punctuation or capitalization _(in order to avoid repetition of words)_.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1601227270130,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "vegNKWAAtL4g"
   },
   "outputs": [],
   "source": [
    " corpus = ['he is a king','she is a queen',\n",
    "    'he is a man','i would like some mango juice',\n",
    "    'she is a woman','i would like some orange juice','i would like some apple juice','nairobi is the capital of kenya',\n",
    "    'delhi is the capital of india','oslo is the capital of norway'   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-m_wMWalS8S"
   },
   "source": [
    "### Vocabulary and Word Indexing\n",
    "\n",
    "\n",
    "*   Splits corpus sentences into individual word sequences\n",
    "*   Generates a vocabulary of all words that occur in the training corpus\n",
    "*   Assigns indices to words\n",
    "*   Converts sequences of words into their respective index sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1601228735046,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "VMDkC4cHUpH-"
   },
   "outputs": [],
   "source": [
    "#returns list of sentences and distinct word vocabulary\n",
    "def split_words(corpus):\n",
    "  tokens = []\n",
    "  vocab=set()\n",
    "  for x in corpus:\n",
    "    sep=x.split(' ')\n",
    "    tokens.append(sep)\n",
    "    vocab=vocab|set(sep)\n",
    "  vocab=list(vocab)\n",
    "  return tokens,vocab\n",
    "sequences,vocab = split_words(corpus)\n",
    "#maps word to index and index to word\n",
    "word_to_ind={}\n",
    "ind_to_word={}\n",
    "ind=0\n",
    "for word in vocab:\n",
    "  word_to_ind[word]=ind\n",
    "  ind_to_word[ind]=word\n",
    "  ind+=1\n",
    "#indexed sequences\n",
    "seq_inds=[[word_to_ind[word] for word in sequence] for sequence in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLh9S1WkmIVE"
   },
   "source": [
    "### Window Selection and Context-Target Generation\n",
    "*  The window size and mode for context words' range can be chosen:-\n",
    "   *   Window mode bi-directional ('bi_dir') looks at words before and after the target word in sequence 2 times that of window size. \n",
    "   *   Window mode uni-directional ('uni_dir') looks at words only before the target word in sequence.\n",
    "*  The loop iterates over the entire corpus and applies the pair_words function on all sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1901,
     "status": "ok",
     "timestamp": 1601230653531,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "dC0M9X2fUwN7"
   },
   "outputs": [],
   "source": [
    "window_size=2\n",
    "mode='bi_dir'  \n",
    "\n",
    "\n",
    "def pair_words_single(seq_ind,window):\n",
    "  pairs=[]\n",
    "  for target_pos in range(len(seq_ind)):\n",
    "    context_start=max(target_pos-window,0)\n",
    "    context_end=target_pos\n",
    "    for context_pos in range(context_start,context_end):\n",
    "      pairs.append((seq_ind[context_pos],seq_ind[target_pos]))\n",
    "  return pairs\n",
    "def pair_words_dual(seq_ind,window):\n",
    "  pairs=[]\n",
    "  for target_pos in range(len(seq_ind)):\n",
    "    context_start=max(target_pos-window,0)\n",
    "    context_end=min(target_pos+window+1,len(seq_ind))\n",
    "    for context_pos in range(context_start,context_end):\n",
    "      if context_pos!=target_pos:\n",
    "        pairs.append((seq_ind[context_pos],seq_ind[target_pos]))\n",
    "  return pairs\n",
    "def pair_words(seq_ind,window_size=2,mode='bi_dir'):       #returns context-target pairs for a sequence of words/indices, given a window size(default=2) and mode(default=bi_dir)\n",
    "  if mode=='uni_dir':\n",
    "    return pair_words_single(seq_ind,window_size)\n",
    "  else:\n",
    "    return pair_words_dual(seq_ind,window_size)\n",
    "\n",
    "all_pairs=[]\n",
    "for seq_ind in seq_inds:                #loop iterates over all indexed sequences to compute all possible target-context pairs wrt each sentence\n",
    "  pairs_per_sequence=pair_words(seq_ind,window_size,mode)\n",
    "  for pair in pairs_per_sequence:\n",
    "    all_pairs.append(pair)\n",
    "all_pairs=np.array(all_pairs)  #context target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAqrFYy_yPM5"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "*   The embedding dimensions, number of epochs and learning rate can be chosen\n",
    "    *  Having many embedding dimensions helps if there is a large vocabulary\n",
    "*   Randomly initializes W1-Embedding matrix, W2-softmax weight\n",
    "*   We won't be multplying the W1 matrix by the one-hot context vector but rather chose the corresponding column from W1 using the word's index since this produces the same effect and is less computationally wasteful\n",
    "*   The F.log_softmax converts activation z2 into a softmax probability output. The  F.nll_loss computes loss of softmax output w.r.t target on-hot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 16302,
     "status": "ok",
     "timestamp": 1601232154529,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "oGjYtCN9xWlX",
    "outputId": "45c7614c-692e-44e6-b29e-139920cb53ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 8.234246253967285\n",
      "Loss at epoch 50: 2.1590919494628906\n",
      "Loss at epoch 100: 1.6593035459518433\n",
      "Loss at epoch 150: 1.5578502416610718\n",
      "Loss at epoch 200: 1.5273125171661377\n",
      "Loss at epoch 250: 1.514283299446106\n",
      "Loss at epoch 300: 1.507468819618225\n",
      "Loss at epoch 350: 1.5033494234085083\n",
      "Loss at epoch 400: 1.5006176233291626\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=20\n",
    "num_epochs = 401\n",
    "learning_rate = 0.003\n",
    "\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "W1 = Variable(torch.randn(embedding_dim, vocab_size).float(), requires_grad=True)  #embedding matrix\n",
    "W2 = Variable(torch.randn(vocab_size, embedding_dim).float(), requires_grad=True)  #weights for softmax layer\n",
    "\n",
    "losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for context, target in all_pairs:\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = W1[:,context]\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    l_epoch=loss_val/len(all_pairs)\n",
    "    losses.append(l_epoch)\n",
    "    if epoch % 50 == 0:                 # displays loss at every 50th epoch\n",
    "        print(f'Loss at epoch {epoch}: {l_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74wnbSX43YB-"
   },
   "source": [
    "### Displaying Loss Vs Epoch Number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1601232418478,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "29WpzwmD4FSf",
    "outputId": "7c8a479c-e941-4d1b-fbbd-9f567248dc55"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([epoch for epoch in range(num_epochs)],losses)\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5j4py8x6g0D"
   },
   "source": [
    "### Displaying the embedding vector\n",
    "* Using pandas dataframe to display the embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 2740,
     "status": "ok",
     "timestamp": 1601232436930,
     "user": {
      "displayName": "Nishant Ghule",
      "photoUrl": "",
      "userId": "18439549493497591936"
     },
     "user_tz": -330
    },
    "id": "XEBjLN0OxfCE",
    "outputId": "136f233f-521a-46f3-d515-5bad5ba90221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queen</th>\n",
       "      <th>india</th>\n",
       "      <th>nairobi</th>\n",
       "      <th>oslo</th>\n",
       "      <th>would</th>\n",
       "      <th>man</th>\n",
       "      <th>king</th>\n",
       "      <th>mango</th>\n",
       "      <th>like</th>\n",
       "      <th>of</th>\n",
       "      <th>norway</th>\n",
       "      <th>she</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>orange</th>\n",
       "      <th>the</th>\n",
       "      <th>juice</th>\n",
       "      <th>i</th>\n",
       "      <th>woman</th>\n",
       "      <th>kenya</th>\n",
       "      <th>capital</th>\n",
       "      <th>some</th>\n",
       "      <th>he</th>\n",
       "      <th>apple</th>\n",
       "      <th>delhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dim1</th>\n",
       "      <td>-0.036161</td>\n",
       "      <td>-0.102818</td>\n",
       "      <td>0.575723</td>\n",
       "      <td>1.819240</td>\n",
       "      <td>0.360302</td>\n",
       "      <td>-1.019552</td>\n",
       "      <td>-0.759313</td>\n",
       "      <td>0.533259</td>\n",
       "      <td>-0.076990</td>\n",
       "      <td>-0.975822</td>\n",
       "      <td>-0.350059</td>\n",
       "      <td>-0.954930</td>\n",
       "      <td>-0.147861</td>\n",
       "      <td>-0.011795</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>-0.861844</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>-1.489596</td>\n",
       "      <td>0.615153</td>\n",
       "      <td>-0.727514</td>\n",
       "      <td>-0.160205</td>\n",
       "      <td>0.213543</td>\n",
       "      <td>0.122754</td>\n",
       "      <td>0.803406</td>\n",
       "      <td>-1.029880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim2</th>\n",
       "      <td>-1.327287</td>\n",
       "      <td>-1.955472</td>\n",
       "      <td>-0.095996</td>\n",
       "      <td>-0.843279</td>\n",
       "      <td>-0.756120</td>\n",
       "      <td>-0.176712</td>\n",
       "      <td>0.193823</td>\n",
       "      <td>1.105958</td>\n",
       "      <td>-0.877038</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.049071</td>\n",
       "      <td>0.499593</td>\n",
       "      <td>2.317803</td>\n",
       "      <td>-0.060683</td>\n",
       "      <td>0.748795</td>\n",
       "      <td>-0.761271</td>\n",
       "      <td>-1.087147</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>-0.358497</td>\n",
       "      <td>-0.159798</td>\n",
       "      <td>0.232910</td>\n",
       "      <td>-0.804367</td>\n",
       "      <td>0.943587</td>\n",
       "      <td>0.847443</td>\n",
       "      <td>0.424028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim3</th>\n",
       "      <td>1.559137</td>\n",
       "      <td>0.792662</td>\n",
       "      <td>0.437843</td>\n",
       "      <td>-0.744809</td>\n",
       "      <td>-1.124589</td>\n",
       "      <td>0.778172</td>\n",
       "      <td>-1.153513</td>\n",
       "      <td>-0.484632</td>\n",
       "      <td>1.441613</td>\n",
       "      <td>-0.914049</td>\n",
       "      <td>-1.027999</td>\n",
       "      <td>-0.903392</td>\n",
       "      <td>-0.324093</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>1.209657</td>\n",
       "      <td>0.258687</td>\n",
       "      <td>-0.074783</td>\n",
       "      <td>0.231863</td>\n",
       "      <td>-0.245299</td>\n",
       "      <td>-0.383043</td>\n",
       "      <td>-0.539475</td>\n",
       "      <td>0.135586</td>\n",
       "      <td>0.145102</td>\n",
       "      <td>0.983340</td>\n",
       "      <td>0.548632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim4</th>\n",
       "      <td>1.084259</td>\n",
       "      <td>1.199160</td>\n",
       "      <td>-1.618858</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>1.857647</td>\n",
       "      <td>-0.985901</td>\n",
       "      <td>0.915909</td>\n",
       "      <td>-1.074010</td>\n",
       "      <td>0.430219</td>\n",
       "      <td>0.801788</td>\n",
       "      <td>-0.742924</td>\n",
       "      <td>0.386327</td>\n",
       "      <td>0.638836</td>\n",
       "      <td>-0.785777</td>\n",
       "      <td>0.851662</td>\n",
       "      <td>1.045429</td>\n",
       "      <td>0.806938</td>\n",
       "      <td>-0.043287</td>\n",
       "      <td>-0.533952</td>\n",
       "      <td>0.202971</td>\n",
       "      <td>-0.956553</td>\n",
       "      <td>-1.167984</td>\n",
       "      <td>-0.516710</td>\n",
       "      <td>-0.111578</td>\n",
       "      <td>-0.850401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim5</th>\n",
       "      <td>0.507743</td>\n",
       "      <td>1.490355</td>\n",
       "      <td>1.059935</td>\n",
       "      <td>1.215951</td>\n",
       "      <td>1.572900</td>\n",
       "      <td>0.257085</td>\n",
       "      <td>-0.615260</td>\n",
       "      <td>-1.005169</td>\n",
       "      <td>1.097573</td>\n",
       "      <td>-0.482634</td>\n",
       "      <td>0.078410</td>\n",
       "      <td>-1.386575</td>\n",
       "      <td>-0.784253</td>\n",
       "      <td>-1.374583</td>\n",
       "      <td>-0.046421</td>\n",
       "      <td>1.286935</td>\n",
       "      <td>0.993532</td>\n",
       "      <td>1.010255</td>\n",
       "      <td>-0.756422</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-1.683317</td>\n",
       "      <td>-0.909158</td>\n",
       "      <td>-1.982054</td>\n",
       "      <td>-0.791789</td>\n",
       "      <td>-1.468674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim6</th>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.467953</td>\n",
       "      <td>1.019413</td>\n",
       "      <td>-1.000317</td>\n",
       "      <td>-0.430615</td>\n",
       "      <td>2.078317</td>\n",
       "      <td>-0.338524</td>\n",
       "      <td>1.235388</td>\n",
       "      <td>1.636325</td>\n",
       "      <td>-1.014420</td>\n",
       "      <td>-0.042448</td>\n",
       "      <td>-0.572622</td>\n",
       "      <td>0.481027</td>\n",
       "      <td>2.318200</td>\n",
       "      <td>-1.618564</td>\n",
       "      <td>2.182865</td>\n",
       "      <td>0.713468</td>\n",
       "      <td>-1.435737</td>\n",
       "      <td>-0.443694</td>\n",
       "      <td>0.984956</td>\n",
       "      <td>0.526437</td>\n",
       "      <td>-0.367494</td>\n",
       "      <td>0.440313</td>\n",
       "      <td>-1.322046</td>\n",
       "      <td>0.849481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim7</th>\n",
       "      <td>0.465516</td>\n",
       "      <td>0.452202</td>\n",
       "      <td>0.651260</td>\n",
       "      <td>0.910312</td>\n",
       "      <td>0.448697</td>\n",
       "      <td>0.765616</td>\n",
       "      <td>1.885428</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.745021</td>\n",
       "      <td>0.736463</td>\n",
       "      <td>0.082485</td>\n",
       "      <td>0.782537</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>-0.336633</td>\n",
       "      <td>1.983537</td>\n",
       "      <td>1.219118</td>\n",
       "      <td>0.298435</td>\n",
       "      <td>0.212607</td>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>0.118877</td>\n",
       "      <td>0.246666</td>\n",
       "      <td>-0.746054</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim8</th>\n",
       "      <td>0.054574</td>\n",
       "      <td>-0.499396</td>\n",
       "      <td>-1.200197</td>\n",
       "      <td>-1.185314</td>\n",
       "      <td>0.075599</td>\n",
       "      <td>0.084761</td>\n",
       "      <td>0.103030</td>\n",
       "      <td>-1.123246</td>\n",
       "      <td>0.178857</td>\n",
       "      <td>0.073867</td>\n",
       "      <td>0.576864</td>\n",
       "      <td>-0.708594</td>\n",
       "      <td>-0.700726</td>\n",
       "      <td>-0.978337</td>\n",
       "      <td>1.557905</td>\n",
       "      <td>0.492451</td>\n",
       "      <td>0.277952</td>\n",
       "      <td>-0.313633</td>\n",
       "      <td>2.475273</td>\n",
       "      <td>0.849869</td>\n",
       "      <td>0.423280</td>\n",
       "      <td>-0.587830</td>\n",
       "      <td>1.666239</td>\n",
       "      <td>0.756231</td>\n",
       "      <td>-0.920931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim9</th>\n",
       "      <td>0.147006</td>\n",
       "      <td>-0.433515</td>\n",
       "      <td>-1.306297</td>\n",
       "      <td>-1.646120</td>\n",
       "      <td>-2.240684</td>\n",
       "      <td>0.483781</td>\n",
       "      <td>-1.271074</td>\n",
       "      <td>-0.951202</td>\n",
       "      <td>-0.417891</td>\n",
       "      <td>0.518677</td>\n",
       "      <td>0.839030</td>\n",
       "      <td>-0.358746</td>\n",
       "      <td>1.614945</td>\n",
       "      <td>-0.734380</td>\n",
       "      <td>-0.062265</td>\n",
       "      <td>-0.486522</td>\n",
       "      <td>-1.202685</td>\n",
       "      <td>0.619786</td>\n",
       "      <td>-0.887143</td>\n",
       "      <td>0.432844</td>\n",
       "      <td>-1.186051</td>\n",
       "      <td>0.182525</td>\n",
       "      <td>-0.685745</td>\n",
       "      <td>-0.368727</td>\n",
       "      <td>-0.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim10</th>\n",
       "      <td>-1.307957</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>1.266431</td>\n",
       "      <td>-0.202127</td>\n",
       "      <td>-0.080148</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>-1.136838</td>\n",
       "      <td>-1.013504</td>\n",
       "      <td>-0.565940</td>\n",
       "      <td>-0.056349</td>\n",
       "      <td>0.317403</td>\n",
       "      <td>-0.250442</td>\n",
       "      <td>1.677602</td>\n",
       "      <td>-0.205736</td>\n",
       "      <td>0.278112</td>\n",
       "      <td>-1.543194</td>\n",
       "      <td>0.301899</td>\n",
       "      <td>-0.552293</td>\n",
       "      <td>1.330651</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.565354</td>\n",
       "      <td>-1.129121</td>\n",
       "      <td>-0.454156</td>\n",
       "      <td>-0.582531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim11</th>\n",
       "      <td>0.104206</td>\n",
       "      <td>-0.131398</td>\n",
       "      <td>-0.092828</td>\n",
       "      <td>0.540732</td>\n",
       "      <td>1.054602</td>\n",
       "      <td>-0.493650</td>\n",
       "      <td>1.093774</td>\n",
       "      <td>-0.053831</td>\n",
       "      <td>-0.490718</td>\n",
       "      <td>-0.362479</td>\n",
       "      <td>0.226025</td>\n",
       "      <td>-1.083955</td>\n",
       "      <td>-0.319413</td>\n",
       "      <td>-2.647613</td>\n",
       "      <td>-0.305389</td>\n",
       "      <td>0.911700</td>\n",
       "      <td>-1.250384</td>\n",
       "      <td>2.872609</td>\n",
       "      <td>1.595800</td>\n",
       "      <td>0.235497</td>\n",
       "      <td>2.252079</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>-0.320494</td>\n",
       "      <td>1.162979</td>\n",
       "      <td>-1.378832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim12</th>\n",
       "      <td>1.647382</td>\n",
       "      <td>1.752843</td>\n",
       "      <td>-0.669433</td>\n",
       "      <td>1.155128</td>\n",
       "      <td>0.650682</td>\n",
       "      <td>2.305120</td>\n",
       "      <td>0.030604</td>\n",
       "      <td>0.326712</td>\n",
       "      <td>-0.241371</td>\n",
       "      <td>1.249708</td>\n",
       "      <td>-0.051684</td>\n",
       "      <td>1.452606</td>\n",
       "      <td>0.619736</td>\n",
       "      <td>-0.173072</td>\n",
       "      <td>0.674521</td>\n",
       "      <td>0.899943</td>\n",
       "      <td>0.258858</td>\n",
       "      <td>-0.125447</td>\n",
       "      <td>2.006434</td>\n",
       "      <td>-0.094690</td>\n",
       "      <td>1.220144</td>\n",
       "      <td>-1.420753</td>\n",
       "      <td>0.818009</td>\n",
       "      <td>0.952994</td>\n",
       "      <td>1.209708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim13</th>\n",
       "      <td>-0.195308</td>\n",
       "      <td>-1.814661</td>\n",
       "      <td>-0.973226</td>\n",
       "      <td>0.357820</td>\n",
       "      <td>-0.539479</td>\n",
       "      <td>-0.547644</td>\n",
       "      <td>0.368185</td>\n",
       "      <td>-1.602725</td>\n",
       "      <td>0.016561</td>\n",
       "      <td>-1.776373</td>\n",
       "      <td>-0.761778</td>\n",
       "      <td>1.226853</td>\n",
       "      <td>1.403087</td>\n",
       "      <td>-1.351974</td>\n",
       "      <td>-1.095022</td>\n",
       "      <td>-2.631216</td>\n",
       "      <td>-1.154096</td>\n",
       "      <td>0.054554</td>\n",
       "      <td>-0.335981</td>\n",
       "      <td>0.312972</td>\n",
       "      <td>1.789903</td>\n",
       "      <td>-0.010728</td>\n",
       "      <td>0.492073</td>\n",
       "      <td>-2.441371</td>\n",
       "      <td>0.251155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim14</th>\n",
       "      <td>0.237227</td>\n",
       "      <td>-0.124323</td>\n",
       "      <td>2.640644</td>\n",
       "      <td>0.073085</td>\n",
       "      <td>0.558330</td>\n",
       "      <td>-1.487971</td>\n",
       "      <td>0.934879</td>\n",
       "      <td>-2.756802</td>\n",
       "      <td>-0.446537</td>\n",
       "      <td>0.878287</td>\n",
       "      <td>-0.137331</td>\n",
       "      <td>1.572019</td>\n",
       "      <td>0.461481</td>\n",
       "      <td>0.112693</td>\n",
       "      <td>-0.813970</td>\n",
       "      <td>-0.338193</td>\n",
       "      <td>-0.843649</td>\n",
       "      <td>0.437436</td>\n",
       "      <td>0.462639</td>\n",
       "      <td>-1.275347</td>\n",
       "      <td>-0.477142</td>\n",
       "      <td>-0.683188</td>\n",
       "      <td>1.898685</td>\n",
       "      <td>-0.729708</td>\n",
       "      <td>0.496161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim15</th>\n",
       "      <td>1.659196</td>\n",
       "      <td>-2.072630</td>\n",
       "      <td>0.088706</td>\n",
       "      <td>-1.416800</td>\n",
       "      <td>1.302166</td>\n",
       "      <td>0.624184</td>\n",
       "      <td>0.044016</td>\n",
       "      <td>-0.402611</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>-0.897401</td>\n",
       "      <td>-1.666120</td>\n",
       "      <td>0.115796</td>\n",
       "      <td>1.144098</td>\n",
       "      <td>0.183380</td>\n",
       "      <td>1.560028</td>\n",
       "      <td>-0.808938</td>\n",
       "      <td>0.964436</td>\n",
       "      <td>0.651407</td>\n",
       "      <td>0.200925</td>\n",
       "      <td>-2.543773</td>\n",
       "      <td>-1.102521</td>\n",
       "      <td>0.516720</td>\n",
       "      <td>-0.162048</td>\n",
       "      <td>-0.298225</td>\n",
       "      <td>0.050147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim16</th>\n",
       "      <td>2.572049</td>\n",
       "      <td>1.506919</td>\n",
       "      <td>-0.925050</td>\n",
       "      <td>0.067892</td>\n",
       "      <td>-1.165766</td>\n",
       "      <td>0.714388</td>\n",
       "      <td>-0.369298</td>\n",
       "      <td>-0.837624</td>\n",
       "      <td>-1.126898</td>\n",
       "      <td>-0.861529</td>\n",
       "      <td>2.083905</td>\n",
       "      <td>1.764822</td>\n",
       "      <td>-0.542066</td>\n",
       "      <td>0.899388</td>\n",
       "      <td>0.219853</td>\n",
       "      <td>-0.242565</td>\n",
       "      <td>-0.628506</td>\n",
       "      <td>-1.504433</td>\n",
       "      <td>0.636894</td>\n",
       "      <td>-0.710008</td>\n",
       "      <td>0.358272</td>\n",
       "      <td>-0.112870</td>\n",
       "      <td>2.182767</td>\n",
       "      <td>-0.490754</td>\n",
       "      <td>-0.033109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim17</th>\n",
       "      <td>0.341409</td>\n",
       "      <td>-0.494868</td>\n",
       "      <td>-0.339836</td>\n",
       "      <td>-0.478505</td>\n",
       "      <td>0.683787</td>\n",
       "      <td>-1.028680</td>\n",
       "      <td>-1.782612</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>1.070114</td>\n",
       "      <td>-0.425698</td>\n",
       "      <td>0.961924</td>\n",
       "      <td>-0.486170</td>\n",
       "      <td>-0.539417</td>\n",
       "      <td>-0.999428</td>\n",
       "      <td>0.786322</td>\n",
       "      <td>-0.818247</td>\n",
       "      <td>1.507360</td>\n",
       "      <td>0.914119</td>\n",
       "      <td>0.112241</td>\n",
       "      <td>-0.130587</td>\n",
       "      <td>0.951903</td>\n",
       "      <td>2.766363</td>\n",
       "      <td>-0.987052</td>\n",
       "      <td>-0.847117</td>\n",
       "      <td>0.028622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim18</th>\n",
       "      <td>0.113623</td>\n",
       "      <td>-0.873141</td>\n",
       "      <td>0.621478</td>\n",
       "      <td>-0.762028</td>\n",
       "      <td>1.015448</td>\n",
       "      <td>-0.020627</td>\n",
       "      <td>-0.713796</td>\n",
       "      <td>-1.052075</td>\n",
       "      <td>1.448579</td>\n",
       "      <td>0.871527</td>\n",
       "      <td>-0.102608</td>\n",
       "      <td>-0.487669</td>\n",
       "      <td>-0.395835</td>\n",
       "      <td>1.389104</td>\n",
       "      <td>0.928878</td>\n",
       "      <td>0.065601</td>\n",
       "      <td>-0.360332</td>\n",
       "      <td>-1.044549</td>\n",
       "      <td>-0.439935</td>\n",
       "      <td>-0.873887</td>\n",
       "      <td>0.983970</td>\n",
       "      <td>-1.074680</td>\n",
       "      <td>-0.200909</td>\n",
       "      <td>0.699972</td>\n",
       "      <td>1.423352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim19</th>\n",
       "      <td>0.318021</td>\n",
       "      <td>0.504512</td>\n",
       "      <td>-1.096416</td>\n",
       "      <td>0.263241</td>\n",
       "      <td>0.888893</td>\n",
       "      <td>-0.368199</td>\n",
       "      <td>-0.617052</td>\n",
       "      <td>-0.696424</td>\n",
       "      <td>0.271722</td>\n",
       "      <td>-1.029491</td>\n",
       "      <td>0.471485</td>\n",
       "      <td>-0.401355</td>\n",
       "      <td>-0.084821</td>\n",
       "      <td>-1.245928</td>\n",
       "      <td>0.379309</td>\n",
       "      <td>0.530345</td>\n",
       "      <td>0.082503</td>\n",
       "      <td>1.328081</td>\n",
       "      <td>-0.669518</td>\n",
       "      <td>1.642801</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>0.071694</td>\n",
       "      <td>-1.012337</td>\n",
       "      <td>0.398128</td>\n",
       "      <td>-1.507010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dim20</th>\n",
       "      <td>0.527636</td>\n",
       "      <td>0.907210</td>\n",
       "      <td>1.345510</td>\n",
       "      <td>1.151857</td>\n",
       "      <td>-0.973533</td>\n",
       "      <td>-0.810088</td>\n",
       "      <td>-0.172803</td>\n",
       "      <td>0.751901</td>\n",
       "      <td>1.133639</td>\n",
       "      <td>0.097372</td>\n",
       "      <td>0.196424</td>\n",
       "      <td>-0.296015</td>\n",
       "      <td>0.191627</td>\n",
       "      <td>-0.773666</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>1.608523</td>\n",
       "      <td>0.803952</td>\n",
       "      <td>-0.239250</td>\n",
       "      <td>-0.327968</td>\n",
       "      <td>-0.548590</td>\n",
       "      <td>0.373116</td>\n",
       "      <td>-0.929441</td>\n",
       "      <td>-0.812840</td>\n",
       "      <td>-1.284642</td>\n",
       "      <td>-0.276259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          queen     india   nairobi  ...        he     apple     delhi\n",
       "dim1  -0.036161 -0.102818  0.575723  ...  0.122754  0.803406 -1.029880\n",
       "dim2  -1.327287 -1.955472 -0.095996  ...  0.943587  0.847443  0.424028\n",
       "dim3   1.559137  0.792662  0.437843  ...  0.145102  0.983340  0.548632\n",
       "dim4   1.084259  1.199160 -1.618858  ... -0.516710 -0.111578 -0.850401\n",
       "dim5   0.507743  1.490355  1.059935  ... -1.982054 -0.791789 -1.468674\n",
       "dim6   0.009620  0.467953  1.019413  ...  0.440313 -1.322046  0.849481\n",
       "dim7   0.465516  0.452202  0.651260  ...  0.246666 -0.746054  0.810700\n",
       "dim8   0.054574 -0.499396 -1.200197  ...  1.666239  0.756231 -0.920931\n",
       "dim9   0.147006 -0.433515 -1.306297  ... -0.685745 -0.368727 -0.619400\n",
       "dim10 -1.307957  0.119826  1.266431  ... -1.129121 -0.454156 -0.582531\n",
       "dim11  0.104206 -0.131398 -0.092828  ... -0.320494  1.162979 -1.378832\n",
       "dim12  1.647382  1.752843 -0.669433  ...  0.818009  0.952994  1.209708\n",
       "dim13 -0.195308 -1.814661 -0.973226  ...  0.492073 -2.441371  0.251155\n",
       "dim14  0.237227 -0.124323  2.640644  ...  1.898685 -0.729708  0.496161\n",
       "dim15  1.659196 -2.072630  0.088706  ... -0.162048 -0.298225  0.050147\n",
       "dim16  2.572049  1.506919 -0.925050  ...  2.182767 -0.490754 -0.033109\n",
       "dim17  0.341409 -0.494868 -0.339836  ... -0.987052 -0.847117  0.028622\n",
       "dim18  0.113623 -0.873141  0.621478  ... -0.200909  0.699972  1.423352\n",
       "dim19  0.318021  0.504512 -1.096416  ... -1.012337  0.398128 -1.507010\n",
       "dim20  0.527636  0.907210  1.345510  ... -0.812840 -1.284642 -0.276259\n",
       "\n",
       "[20 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "embedding_matrix=np.array(W1.data)\n",
    "\n",
    "dfl=pd.DataFrame(embedding_matrix)\n",
    "dfl.columns=[ind_to_word[i] for i in range(vocab_size)]\n",
    "dfl.index=['dim'+str(i+1) for i in range(embedding_dim)]\n",
    "print('Embedding Matrix')\n",
    "dfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOtcvOpU5oIm"
   },
   "source": [
    "### Conclusion:\n",
    "If you've used the default/pre-defined parameters you might have noticed that the embedding vector isn't quite accurate. This is because, the model is limited in it's ability to generalise over a large corpus of training sentences.\n",
    "\n",
    "This can be understood by the fact even us humans would find it hard to pick up similar words and analogies given a limited set of sentences from a completely unfamiliar language.\n",
    "\n",
    "So I hope this notebook along with the Nishant_NLP_Word2Vec.md (Link: https://github.com/Nishant11769/Nishant_Word2Vec/blob/master/Nishant_NLP_Word2Vec.md) file has helped you gain a theoritical and practical understanding on how to implement the Word2Vec model.\n",
    "\n",
    "Thank You!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRVxHGO7y2mp8TjoPQOP8G",
   "collapsed_sections": [],
   "name": "Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
