{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.8 64-bit",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e2d4075660d1d643d1328920576ea01a1bd48051b463c92d735ed4cbd11b4f6e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this Notebook we will learn how to implement subsampling in a word2vec model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Importing The Necessary Stuff"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "source": [
    "# Necessary Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "source": [
    "# Downloading The Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'http://mattmahoney.net/dc/text8.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "        "
   ]
  },
  {
   "source": [
    "# Reading words from the given file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
    }
   ],
   "source": [
    "words=[]\n",
    "with open('data/text8') as f:\n",
    "   \n",
    "    # reading each line     \n",
    "    for line in f: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total words: 17005207\nUnique words: 253854\n"
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "source": [
    "# Creating Dictionaries for Simplicity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is done using `create_lookup_tables` function which we created above.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "source": [
    "# Subsampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Words that show up often such as \"a\", \"an\", \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word  wi  in the training set, we'll discard it with probability given by:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"http://www.sciweavers.org/tex2img.php?eq=P%28wi%29%3D1-%20%20%5Csqrt%7Bt%2Ff%28wi%29%7D%20&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0\" align=\"center\" border=\"0\" alt=\"P(wi)=1-  \\sqrt{t/f(wi)} \" width=\"175\" height=\"26\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "where  t  is a threshold parameter and  f(wi)  is the frequency of word  wi  in the total dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "sampled_words=[int_to_vocab[train_word] for train_word in train_words]"
   ]
  },
  {
   "source": [
    "# Get Sampled words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['anarchism', 'originated', 'term', 'radicals', 'including', 'diggers', 'revolution', 'sans', 'culottes', 'revolution', 'pejorative', 'violent', 'destroy', 'organization', 'society', 'taken', 'self', 'anarchists', 'word', 'anarchism', 'derived', 'archons', 'political', 'belief', 'rulers', 'unnecessary', 'abolished', 'differing', 'anarchism', 'social']\n"
    }
   ],
   "source": [
    "print(sampled_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total words: 4981605\n"
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(train_words)))"
   ]
  },
  {
   "source": [
    "As you can see here not only the no. of total words have decreased, but most of the redundant words have been removed.\n",
    "This will make the model more suitable to use since the model will now be faster and will have less bias."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}